---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
house= read.csv('kc_house_data.csv',TRUE,",")
df<-subset(house,select=-c(id))
head(df)
```
Appropriate function to extract the year and month into separate variables
```{r}

df$year=substr(df$date,0,4)

df$month=substr(df$date,5,6)

df<-subset(df,select=-c(date))


```


Run the models 
```{r}

null_model<-lm(price~1,data=df)
summary(null_model)

full_model<-lm(price~.,data=df)
summary(full_model)

step(null_model, scope = list(lower = null_model, upper = full_model), direction = "forward")

```

sqft_lot is an important predictor for the price of a home, but can you explain why lat (East-West) is such a high predictor for the model? Conversely, why do you think long is such a low predictor?

Answer: As we can see that the AIC value of lat(535838) is much lower then long which is why it is being taken as a high predictor. As soon as sqft_living + lat become a predictor in full model the long value goes very low.




My top 10 predictors for the  linear mode are (sqft_living + lat + view + grade + yr_built + waterfront + bedrooms + bathrooms + zipcode + long)
The R^2 for this model is 0.7051 which is not a good R^2 value but still better than the intial model. It's telling that the predictors for this model are not efficient or relevant to predict the outcome price.
```{r}
lm1<-lm(price~ (sqft_living:sqft_living15 + lat:long + view + grade + yr_built + waterfront + bedrooms + bathrooms + zipcode+year ),data=df)
summary(lm1)
```


Convert the variable zipcode from numeric to a factor variable.The value of R^2 changed and the predictiors changed as well when the zipcode is converted into factor.

```{r}
df$zipcode<-as.factor(df$zipcode)
```


Running model again to find top 10 predictors 
```{r}
null_model<-lm(price~1,data=df)
summary(null_model)

full_model<-lm(price~.,data=df)
summary(full_model)

step(null_model, scope = list(lower = null_model, upper = full_model), direction = "forward")

```

linear regression model 2

```{r}
lm2<-lm(price~ (sqft_living + zipcode + waterfront + grade + view + yr_built + 
                  bedrooms + sqft_above + floors + condition),data=df)
summary(lm2)

```
The zipcode now becomes the one of the top 10 predictors because its not numeric but a factor variable now.

The value of R^2 is 0.806 which shows that the model is somewhat efficient and can be further improved by finding out more imortant predictors than the current ones. Most of the predictors in the model are now highly siginificat


Does the model suffer from heteroskedesticity? (Use bptest in the lmtest package in R. In Python, use het_breuschpagan test in the  statsmodels package)

Answer: Yes the model suffers from heteroskedesticity since the value is less than 0.05
```{r}
lmtest::bptest(lm2)
```

Are there nonlinearities in the model? (Use the plots to discern this)
Answer: Yes there are nonlinerities in the model. The trend lines are horizontal rather than vertical which is an indication of non linearities.

Are the residuals normally distributed?
Answer: No the residuals are not normally distributed as we can see from the plot that values are not properely distributed.
```{r}
par(mfrow=c(2,2)) # init 4 charts in 1 panel
plot(lm2)

```



------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Section 3 - Extra Credit

```{r}

df$bedrooms<-as.factor(df$bedrooms)
null_model<-lm(price~1,data=df)
summary(null_model)

full_model<-lm(price~.,data=df)
summary(full_model)

step(null_model, scope = list(lower = null_model, upper = full_model), direction = "forward")


```

```{r}

library(MASS)
ind <- sapply(df, is.numeric)
df[ind] <- lapply(df[ind], scale)

```



```{r}


lm4<-lm(price~ ( grade + zipcode + sqft_living + waterfront + view + condition + 
    year + yr_renovated + sqft_above + bedrooms:bathrooms ),data=df)
summary(lm4)

```


```{r}
library(MASS)
ind <- sapply(df, is.numeric)
df[ind] <- lapply(df[ind], scale)
```




```{r}
library(rpart)
library(rpart.plot)
regTree <- rpart(price ~ .+ sqft_living + zipcode + waterfront + view + yr_built+yr_renovated + bedrooms+bathrooms + sqft_above + condition +floors, data= df, method = "anova")

```

Regression tree 
```{r}
plot(regTree,uniform = TRUE,main=" Regression tree")
text(regTree,use.n=TRUE,cex=.6)
```

```{r}
par(mfrow=c(1,2))
rsq.rpart(regTree)
```


Compare the output between these two methods. Is there one that you would choose over the other?

Answer: From this, we get to know thar the std error for regrssion trees is 0.31 which less than that of the Linear regression model which is 0.40. Thus, regression tree model has reduced its error & is preffered to the other.


------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

                                           # PART 2
                                            
                                          

```{r}
Auto= read.csv('Auto.csv')
```


Creating a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function.
```{r}
library(MASS)
library(ISLR)
Auto$mpg01 <- ifelse(Auto$mpg > median(Auto$mpg),1,0)
```

Exploring the data graphically in order to investigate the association between mpg01 and the other features.
```{r}
cor(Auto[,-9])
#Scatterplot matrix
pairs(Auto[,-9])
```
in scatter plot horsepower and weight have high significance with year

```{r}
#Boxplots
par(mfrow=c(2,3))
boxplot(cylinders ~ mpg01, data = Auto, main = "Cylinders vs mpg01")
boxplot(displacement ~ mpg01, data = Auto, main = "Displacement vs mpg01")
boxplot(horsepower ~ mpg01, data = Auto, main = "Horsepower vs mpg01")
boxplot(weight ~ mpg01, data = Auto, main = "Weight vs mpg01")
boxplot(acceleration ~ mpg01, data = Auto, main = "Acceleration vs mpg01")
boxplot(year ~ mpg01, data = Auto, main = "Year vs mpg01")

```
some association between “mpg01” and “cylinders”, “weight”, “displacement” and “horsepower” and acceleration look the most promissing. 




```{r}
# splitting the train and test set into 80% and 20%
set.seed(1)
rows <- sample(x=nrow(Auto), size=.80*nrow(Auto))
trainset <- Auto[rows, ]
testset <- Auto[-rows, ]

```



```{r}
# LDA
library(MASS)
lda.fit <- lda(mpg01 ~ (displacement+acceleration+horsepower:year+weight+cylinders), data=trainset)
lda.pred <- predict(lda.fit, testset)
table(testset$mpg01, lda.pred$class)

```


```{r}
round(sum(lda.pred$class!=testset$mpg01)/nrow(testset)*100,2)

```
Test errror is 3.8%

```{r}
#logistic regression
lr.fit <- glm(as.factor(mpg01) ~ (displacement+acceleration+horsepower:year+weight+cylinders), data=trainset, family="binomial")
lr.probs <- predict(lr.fit, testset, type="response")
lr.pred <- ifelse(lr.probs>0.5, "1", "0")
table(testset$mpg01, lr.pred)
```


```{r}
# test-error
round(sum(lr.pred!=testset$mpg01)/nrow(testset)*100,2)
```
Test errror is 6.33%


Performing KNN on the training data
```{r}
data = scale(Auto[,-c(9,10)])
set.seed(1234)
train <- sample(1:dim(Auto)[1], 392*.7, rep=FALSE)
#train <- sample(1:dim(Auto)[1], dim(Auto)[1]*.7, rep=FALSE)
test <- -train
training_data = data[train,c("cylinders","weight","displacement","horsepower","year","acceleration")]
testing_data = data[test,c("cylinders","weight","displacement","horsepower","year","acceleration")]
## KNN take the training response variable seperately
train.mpg01 = Auto$mpg01[train]

## we also need the have the testing_y seperately for assesing the model later on
test.mpg01= Auto$mpg01[test]
```



```{r}
library(class)
set.seed(1234)
knn_pred_y = knn(training_data, testing_data, train.mpg01, k = 1)
table(knn_pred_y, test.mpg01)

mean(knn_pred_y != test.mpg01)

#Using a for loop to find the optimum K value
knn_pred_y = NULL
error_rate = NULL
for(i in 1:dim(testing_data)[1]){
  set.seed(1234)
  knn_pred_y = knn(training_data,testing_data,train.mpg01,k=i)
  error_rate[i] = mean(test.mpg01 != knn_pred_y)
}

```


```{r}
### find the minimum error rate
min_error_rate = min(round((error_rate)*100,2))
print(min_error_rate)
```

The minimum error rate is 5.93%
```{r}
### get the index of that error rate, which is the k
K = which(error_rate == min_error_rate)
print(K)
# When we train a KNN model with k=3, then we get the lowest misclassification error rate of 5.93%.
library(ggplot2)
qplot(1:dim(testing_data)[1], error_rate, xlab = "K",
      ylab = "Error Rate",
      geom=c("point", "line"))

```
Which value of K seems to perform the best on this data set?
Answer:3
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

                                            PART 3
                                            
                                            DBSCAN


```{r}
data<-read.csv('clustering1.csv')
df2<-read.csv('clustering2.csv')
df3<-read.csv('clustering3.csv')
df4<-read.csv('clustering4.csv')
```
```{r}
library(dbscan)
library("fpc")
set.seed(1234)
db1<-fpc::dbscan(data,eps=2,MinPts=80)
db2<-fpc::dbscan(df2,eps=0.5,MinPts=10)
db3<-fpc::dbscan(df3,eps=0.15,MinPts=5)
db4<-fpc::dbscan(df4,eps=0.18,MinPts=10)
print(db1)
print(db2)
print(db3)
print(db4)
```

```{r}
library("factoextra")
dp1<-fviz_cluster(db1,data=data,stand=FALSE,ellipse = FALSE,show.clust.cent = FALSE,
                  geom="point",palette="jco",ggtheme=theme_classic())
dp2<-fviz_cluster(db2,data=df2,stand=FALSE,ellipse = FALSE,show.clust.cent = FALSE,
                  geom="point",palette="jco",ggtheme=theme_classic())
dp3<-fviz_cluster(db3,data=df3,stand=FALSE,ellipse = FALSE,show.clust.cent = FALSE,
                  geom="point",palette="jco",ggtheme=theme_classic())
dp4<-fviz_cluster(db4,data=df4,stand=FALSE,ellipse = FALSE,show.clust.cent = FALSE,
                  geom="point",palette="jco",ggtheme=theme_classic())

library(gridExtra)
grid.arrange(dp1,dp2,dp3,dp4,nrow=2)
```



------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Hierarchical clustering


```{r}
data <- na.omit(data)
data <- scale(data)
head(data)
```

```{r}
d <- dist(data, method = "euclidean")
hc1 <- hclust(d, method = "complete" )
plot(hc1, cex = 0.6, hang = -1)
```

```{r}
segtree<-cutree(hc1,k=3)
table(segtree)

```

```{r}
plot(hc1,cex=0.6)
rect.hclust(hc1,k=3,border=2.5)
fviz_cluster(list(data=data,cluster=segtree))
```






```{r}
d1 <- dist(df2, method = "euclidean")
hc2 <- hclust(d1, method = "complete" )
plot(hc2, cex = 0.6, hang = -1)

segtree1<-cutree(hc2,k=3)
table(segtree1)

plot(hc2,cex=0.6)
rect.hclust(hc2,k=3,border=2.5)
fviz_cluster(list(data=df2,cluster=segtree1))

```



```{r}
d3 <- dist(df3, method = "euclidean")
hc3 <- hclust(d3, method = "complete" )
plot(hc3, cex = 0.6, hang = -1)

segtree3<-cutree(hc3,k=3)
table(segtree3)

plot(hc3,cex=0.6)
rect.hclust(hc3,k=3,border=2.5)
fviz_cluster(list(data=df3,cluster=segtree3))

```


```{r}
d4 <- dist(df4, method = "euclidean")
hc4 <- hclust(d4, method = "complete" )
plot(hc4, cex = 0.6, hang = -1)

segtree4<-cutree(hc4,k=3)
table(segtree4)

plot(hc4,cex=0.6)
rect.hclust(hc4,k=3,border=2.5)
fviz_cluster(list(data=df4,cluster=segtree4))
```



------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

K-MEANS

```{r}
k1<-kmeans(data,centers = 4,nstart = 25)
k2<-kmeans(df2,centers = 8,nstart = 25)
k3<-kmeans(df3,centers = 15,nstart = 25)
k4<-kmeans(df4,centers = 20,nstart = 25)
```

```{r}
pt1<-fviz_cluster(k1,geom="point",data=data)+ggtitle("Kmeans - Clustering 1 dataset")
pt2<-fviz_cluster(k2,geom="point",data=df2)+ggtitle("Kmeans - Clustering 2 dataset")
pt3<-fviz_cluster(k3,geom="point",data=df3)+ggtitle("Kmeans - Clustering 3 dataset")
pt4<-fviz_cluster(k4,geom="point",data=df4)+ggtitle("Kmeans - Clustering 4 dataset")
```

```{r}
library(gridExtra)
grid.arrange(pt1,pt2,pt3,pt4,nrow=2)
```


------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

                                        PART 4
1.Suppose we have a dataset with five predictors, X1=GPA, X2=IQ, X3=Gender (1 for Female, 0 for Male), X4=Interaction between GPA and IQ, and X5=Interaction between GPA and Gender. The response is the starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get β0^=50, β̂ 1=20, β̂ 2=.07, β̂ 3=35, β̂ 4=0.01, and β̂ 5
For a fixed value of IQ and GPA, males earn more on average than females.
For a fixed value of IQ and GPA, females earn more on average than males.
For a fixed value of IQ and GPA, males earn more on average than females provided that the GPA is high enough.
For a fixed value of IQ and GPA, females earn more on average than males provided that the GPA is high enough.

Answer: The correct answer is (iii) i.e. For a fixed value of IQ and GPA,males earn more on average than females provided that the GPA is high enough.If males are 0 and females are 1, then male is the baseline. It's  clear from the sign of β3 that on average women earn more than men if both have zero GPA and zero IQ. However, as GPA increases, average wages become relatively higher for men (β5<0). Therefore, if GPA is high enough, men will earn more than women, on average. if the gpa is low answer b is correct.


B)Predict the salary of a female with IQ of 110 and a GPA of 4.0.

Answer: The formala for linear regression is Y = β[0] +β[1]X +β[2]X^2 +β[3]X^3 + e

which is equal to 85+10GPA+0.07IQ+0.01GPA*IQ

Y=85+10*4+0.07*110+0.01*4*110
y=85+40+7.7+4.4
y=137.1 which is $137100 for the female's starting salary!



Suppose we collect data for a group of students in a statistics class with variables X1=hours studied, X2=undergrad GPA, and Y=receive an A. We fit a logistic regression and produce estimated coefficient, β̂ 0=−6, β̂ 1=0.05, β̂ 2=1.
Estimate the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class.

Answer:
Given:
 
β0 = -6
β1 = 0.05
β2 = 1
and X1=40 & X2=3.5
Probability = (exp(1)^(β0 + (β1*40) + (β2*3.5)))/(1+exp(1)^(β0 + (β1*40) + (β2*3.5))) 
Probability= 37.75%

How many hours would the student in part (a) need to study to have a 50 % chance of getting an A in the class?

Answer:
To increase the chance of A without alter the GPA, the student have to increase the number of hours, so we test a sequence of hours and see how the chances change. Doing the solution we find that x1 equal to 50 hours, therefore to have 50% of chance, he needs to study at least 50 hours.
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
```{r}
n=6
x=matrix(c(1,4,1,3,0,4,5,1,6,2,4,0),nrow=n,byrow=T)
plot(x)
```


Randomly assign a cluster label to each observation. Report the cluster labels for each observation.

```{r}
set.seed(1)
labels <- sample(2, nrow(x), replace = T)
labels
```

```{r}
plot(x, col = (labels + 1), pch = 20, cex = 2)
```
Compute the centroid for each cluster.


```{r}
centroid1 <- c(mean(x[labels == 1, 1]), mean(x[labels == 1, 2]))
centroid2 <- c(mean(x[labels == 2, 1]), mean(x[labels == 2, 2]))
plot(x[,1], x[,2], col=(labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
```


Assign each observation to the centroid to which it is closest, in terms of Euclidean distance. Report the cluster labels for each observation.

```{r}
labels <- c(1, 1, 1, 2, 2, 2)
plot(x[, 1], x[, 2], col = (labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
```
Repeat (c) and (d) until the answers obtained stop changing.
```{r}
centroid1 <- c(mean(x[labels == 1, 1]), mean(x[labels == 1, 2]))
centroid2 <- c(mean(x[labels == 2, 1]), mean(x[labels == 2, 2]))
plot(x[,1], x[,2], col=(labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
```


In your plot from (a), color the observations according to the clusters labels obtained.
```{r}
plot(x[, 1], x[, 2], col=(labels + 1), pch = 20, cex = 2)
```


R

```{r}
library(nnet)
data(iris)
```



```{r}
ggplot(data, aes(x = Petal.Length, y = Sepal.Length, colour = Species)) + 
  geom_point() +
  ggtitle('Iris Species by Petal and Sepal Length')
```


```{r}
iris$y <- ifelse(iris$Species == 'setosa', 0, 1)
mdl31 <- glm(y ~ Sepal.Length, data = iris, subset = (Species != 'virginica'), family = binomial)
summary(mdl31)

mdl32 <- glm(y ~ Sepal.Length, data = iris, subset = (Species != 'versicolor'), family = binomial)
summary(mdl32)

```

```{r}
#PCA result should only contains numeric values 
autoplot(prcomp(df), data = iris, colour = 'Species')

```

```{r}
#Passing label = TRUE draws each data label using rownames
autoplot(prcomp(df), data = iris, colour = 'Species', label = TRUE, label.size = 3)

#Passing shape = FALSE makes plot without points. In this case, label is turned on unless otherwise specified
autoplot(prcomp(df), data = iris, colour = 'Species', shape = FALSE, label.size = 3)

```


```{r}
#Passing loadings = TRUE draws eigenvectors.
autoplot(prcomp(df), data = iris, colour = 'Species', loadings = TRUE)
```
